
// This is an autogenerated file from running `firebase genkit:flow`.
'use server';

/**
 * @fileOverview AI tool to suggest learning milestones based on user-defined goals.
 *
 * - suggestLearningMilestones - A function that suggests learning milestones.
 * - SuggestLearningMilestonesInput - The input type for the suggestLearningMilestones function.
 * - SuggestLearningMilestonesOutput - The return type for the suggestLearningMilestones function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import { routeQueryToModel, type UserQueryInput } from './route-query-flow'; // Import the router

const SuggestLearningMilestonesInputSchema = z.object({
  goal: z.string().describe('The overall learning goal of the user.'),
  currentSkills: z.string().describe('The current skills of the user.'),
  learningPreferences: z.string().describe('The learning preferences of the user.'),
});

export type SuggestLearningMilestonesInput = z.infer<typeof SuggestLearningMilestonesInputSchema>;

const SuggestLearningMilestonesOutputSchema = z.object({
  milestones: z.array(z.string()).describe('A list of suggested learning milestones.'),
  modelUsed: z.string().optional().describe('The model that was used to generate the milestones.'),
  routingReason: z.string().optional().describe('Reason for choosing the model.'),
});

export type SuggestLearningMilestonesOutput = z.infer<typeof SuggestLearningMilestonesOutputSchema>;

export async function suggestLearningMilestones(input: SuggestLearningMilestonesInput): Promise<SuggestLearningMilestonesOutput> {
  return suggestLearningMilestonesFlow(input);
}

// Note: The 'model' field in definePrompt sets a default if not overridden at call time.
// We will override it dynamically based on the router's decision.
const suggestLearningMilestonesPrompt = ai.definePrompt({
  name: 'suggestLearningMilestonesPrompt',
  input: {schema: SuggestLearningMilestonesInputSchema},
  output: {schema: SuggestLearningMilestonesOutputSchema.omit({ modelUsed: true, routingReason: true })}, // Prompt's direct output schema
  prompt: `You are an AI learning assistant. Your goal is to help users achieve their learning goals by suggesting a list of milestones.

  Consider the user's current skills, learning preferences, and overall goal when suggesting milestones.

  Goal: {{{goal}}}
  Current Skills: {{{currentSkills}}}
  Learning Preferences: {{{learningPreferences}}}

  Suggest a list of milestones to achieve the goal. Ensure the output is an array of strings under the 'milestones' key.
  `,
});

const suggestLearningMilestonesFlow = ai.defineFlow(
  {
    name: 'suggestLearningMilestonesFlow',
    inputSchema: SuggestLearningMilestonesInputSchema,
    outputSchema: SuggestLearningMilestonesOutputSchema,
  },
  async (input) => {
    // 1. Route the query to decide which model to use
    const routerInput: UserQueryInput = { queryText: `Suggest learning milestones for the goal: "${input.goal}" considering current skills: "${input.currentSkills}" and preferences: "${input.learningPreferences}"` };
    const modelDecision = await routeQueryToModel(routerInput);
    const selectedModelName = modelDecision.modelName;
    const routingReason = modelDecision.reason;

    console.log(`SuggestMilestones: Routed to use model: ${selectedModelName} because: ${routingReason}`);

    // 2. Call the main prompt using the selected model
    const {output} = await suggestLearningMilestonesPrompt(input, { model: selectedModelName as any }); // Cast to any if specific model type isn't directly compatible
    
    if (!output) {
      // Handle case where prompt might not return expected output
      return { milestones: ["Failed to generate milestones."], modelUsed: selectedModelName, routingReason };
    }
    
    return { ...output, modelUsed: selectedModelName, routingReason };
  }
);
